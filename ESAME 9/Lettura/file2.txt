\documentclass[10pt]{article}

\usepackage{epsfig}
\newcommand{\done}{\rule{2mm}{2mm} \vskip \belowdisplayskip}

\tolerance=10000
\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{0in}
\setlength{\leftmargin}{0in}
\setlength{\textwidth}{7.0in}
\setlength{\textheight}{9.8in}
\hoffset=-1truecm
\voffset=-1.9truecm
\raggedbottom
\def\e {\`e }
\def\a {\`a }
\def\ii {\`\i }
\def\o {\`o }
\def\u {\`u }
\def\E {\`E }
\def\A {\`A }
\def\II {\`\I }
\def\O {\`O }
\def\U {\`U }
\begin{document}
\newcommand{\elimina}[1]{}
\newtheorem{lemma}{Lemma}
\newtheorem{ese}{Esercizio}
\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{observation}{Observation}
\newtheorem{proposition}{Propositon}
\newcommand{\w}{{\bf w}}
\newcommand{\inat}
{{\bf I}\!{\bf I}\mskip -7mu{\bf N}}
\newcommand{\x}{{\bf a}}
\newcommand{\y}{{\bf b}}
\newcommand{\F}{{\cal F}}
\newcommand{\R}{{\sf R\hspace*{-0.9ex}\rule{0.15ex}%
       {1.5ex}\hspace*{0.9ex}}}
\newcommand{\N}{{\sf N\hspace*{-1.0ex}\rule{0.15ex}%
       {1.3ex}\hspace*{1.0ex}}}
\newcommand{\Q}{{\sf Q\hspace*{-1.1ex}\rule{0.15ex}%
       {1.5ex}\hspace*{1.1ex}}}
\newcommand{\C}{{\sf C\hspace*{-0.9ex}\rule{0.15ex}%
       {1.3ex}\hspace*{0.9ex}}}

\newcommand{\proof}[1]{
{\noindent {\bf Proof.} {#1} \hfill\done}}


\section{OR-Decision tables and Decision Trees}

Given a set of conditions and the corresponding actions to be performed according to the outcome of these testing conditions, we can arrange them in a tabular form ${\cal T}$ called a {\em decision table}: each row corresponds to a particular outcome fot the conditions and is called {\em rule}, each column corresponds to a particular action to be performed. A given entry ${\cal T}[i,j]$ of the table is set to one if the action corresponding to column $j$ might be performed given the outcome of the testing condition as in row $i$; the entry is set to zero otherwise.  Different rules might have different probability to occur and testing conditions might be more o less expensive to test. The order in which conditions are tested might be influent or not. 
Decision tables are usually used to described the behaviour of a system whose state can be represented as a vector, the outcome of testing certain conditions. Given a particular state, the system evolves by performing a set of actions that depends on the given state of the system. The state is described by a particular rule, the action by the corresponding row of the table. 

There are different kind of decision tables, according to the system they have to describe: a table is said to be {\em limited} if the outcome of the testing conditions is binary and {\em extended} otherwise; a table in which there is a row for any rule ({\em i.e.}, having $k^L$ rows for $L$ conditions assuming $k$ different possible valuse) is said to be an {\em expanded} decision table, if a row might represent a set of rules that decide a common action the table is said to be {\em compressed}; a decision table is said to  be an {\bf AND}-decision table if {\bf all} the actions in a row must be executed when the corresponding rule occurs;  a decision table is said to  be an {\bf OR}-decision table if {\bf any} of the actions in a row might be executed when the corresponding rule occurs. 



The great advantage of decision tables is that they are easy to read and understand,  and  that they can be automatically converted into a program by means of a decsion tree: a tree that specify the order in which  to test the conditions (depending on the outcome of  previous tests) to decide which actions to execute. Changing the order in which conditions are tested might lead to more or less tests (and hence, higher or lower cost) to be performed to decide which actions to execute. The optimal decision tree is the one that requires the minimum cost when deciding which actions execute. 

Schumacher in \cite{} proposed a Dynamic Programming technique which guaranties to find the optimal decision tree given a limited decision table in which each row contains only one non-zero value. This strategy can be used for any limited {\bf AND}-decision table, defining a new {\em composed action} for any different row vector and substituting single action colums with composed actions. In this way  we obtain a limited {\bf AND}-decision table in which each row has only one non-zero value. Lew in \cite{} gives a Dynamic Programming approach for the case of extended and/or compressed {\bf AND}-decision tables.  

\subsection{Preliminaries and notation}
In this paper we will take into consideration limited {\bf OR}-decision tables and we will assume that the conditions are independent, {\em i.e.}, there are no constraint on the order in which conditions are tested.  We will give the full algorithm and correctness proof for the case of expanded tables (any compressed tabel can be easily expanded).

We will use the following notation:

\begin{itemize}
\item $C= \{ c_1, \ldots, c_L \} $ boolean conditions; the cost of testing condition $c_i$ is given by $w_i >0$; 
\item $R = \{ r_1, \ldots, r_{2^L} \}$ rules; each rule is a boolean vector of $L$ elements, element $i$ corresponding to the outcome of condition $c_i$; the probability that rule $r$ occurres is given by $p_r \geq 0$;
\item $A =\{ a_1, \ldots, a_M \}$ actions;  rule $r_i$ is associated to a non empty subset $A^{(i)} \subseteq A$ to be executed  when the outcome of conditions $c_j$s identifies rule $r_i$.
\end{itemize}

We wish to determine an efficient way to test conditions $c_1, \ldots, c_L$ in order to decide {\em as soon as possible} which action to be executed according to the values of conditions $c_i$s. In particular, we wish to determine in which order conditions $c_i$s have to be checked, so that the minimum number of tests are performed to determine which actions has to be taken (bla bla). 

%To this aim we build what is called a decision tree: a binary tree that ....
We give an algorithm that builds an optimal decision tree given an  {\bf OR}-decision table; the same algorithm can be applied to the case in which rows contain only one non-zero entry (as a special case of  as  {\bf OR}-decision tables) and, hence, also to  {\bf AND}-decision tables in which single actions have been replaced by composed actions.

\bigskip

Before formally defining what a decision tree is, we introduce the concept of $k-cube$: 

\begin{definition}[k-cube]
A {\em k-cube} $K \subseteq R$ is a subset of the set of rules in which the value of $L-k$ conditions is fixed. It can be represented as
a $L$-vector of $k$ dashes ($-$) and $L-k$ values 0's and 1's . We will denote the set of positions containing dashes as $D_K$, and $[1..L]\setminus D_K$ as $V_K$.
 The set of actions $A_K = \cap_{r \in K} A^{(r)}$  is associated to cube $K$ (might be an empty set).  
\end{definition}


\begin{observation}
If $A_K \not= \emptyset$ any action in $A_K$ might be executed in association to any rule in $K$ ({\em i.e.}, there is no need to distinguish between rules in $K$ to decide the action to be performed), hence there is no need to test conditions $c_i$s corresponding to dashes, to decide which actions might be executed. 
\end{observation}
  
\bigskip

\begin{definition}[Decision Tree]
Given $k$-cube $K$, with dashes in positions in $D_K \subseteq [1..L]$ and values $0$s and $1$s in $V_k\subseteq [1..L]$, $k \leq L$, a {\em Decision Tree} for $K$ is a binary tree $T$ with the following properties: 
\begin{enumerate}
\item Each leaf $\ell$ corresponds to a set of rules $K_{\ell} \subseteq K$ and $K_{\ell}$ is a $k'$-cube, with $0\leq k'\leq k$.  Given the set of leaves $\ell_1, \ldots, \ell_n$ of the tree, $1\leq n \leq 2^L$, then $\{ K_{\ell_1} , \ldots,  K_{\ell_n}\}$  is a partition of $K$ into cubes.  
\item Each internal node $v_i$ corresponds to a testing condition $c_{j_i} \in C$ for some $j_i \in V_K$. 
\item Arcs are labeled with one value in $\{ 0, 1 \}$.
\item Each root-leaf path $v_1,\ldots, v_m$ leading to leaf $\ell$, corresponding to cube $K_{\ell}$ of dimension $k'\leq k$, is identifed by an ordered sequence of pairs 
$$ (c_{j_1},o_1),(c_{j_2},o_2), \ldots, (c_{j_m},o_m)$$
where, $1\leq m \leq L-k'$,  $c_{j_i} \in C$ be the condition associated to node $v_i$ and $o_i$ is the label of the outgoing arc on the path. 
 Moreover, the following properties hold:
\begin{enumerate}
\item[I.] $c_{j_i} \not= c_{j_t}$, for every $i \not= t$, $i, t \in \{ 1, \ldots, m\}$;  {\em i.e.}, the testing conditions associated to nodes on the path are distinct. 
\item[II.] The set of rules associated to leaf $\ell$ is the cube having: (a) values $0$'s and $1$'s setted as the labels on the root-leaf path for the corresponding $c_{j_i}$s, (b) conditions in positions $V_K$ setted as in the cube $K$ and (c) dashes in the remaining positions ({\em i.e.}, those in $D_K \setminus \{ j_1, \ldots, j_m\}$) .
\end{enumerate}
\item Each leaf $\ell$ is associated to the set of actions $A_{K_{\ell}}$,  those associated to cube $K_{\ell}$.
\item The decision tree is associated to the intersection of the actions associated to the leaves of the tree; {\em i.e.},  $A_T = \cap_{\ell \in {\cal L}} A_{K_{\ell}}$, where ${\cal L}$ is the set of leaves of the tree. 
\end{enumerate}

\end{definition}

\begin{definition}[Path cost]
Given a Decision Tree $T$ for a cube $K$, let ${\cal P_{\ell}} = v_1,\ldots, v_m$ be a root-leaf path leading to leaf $\ell$, $1\leq m\leq |D_K|$, and let $c_{j_i}$ be the condition associated to node $v_i$, $i=1,\ldots, m$. The cost of the path leading to leaf $\ell$ is defined as the sum of the costs of the conditions corresponding to nodes on ${\cal P_{\ell}}$: 
$$ path(\ell) = \sum_{i =1}^m w_{j_i}.$$
\end{definition}

\begin{definition}[Occurence probability of a Decision Tree]
The occurence probability $P_T$ of a decision tree $T$  for a $k$-cube $K$ is the  probability $P_K$ of any rule in $K$ to occur, {\em i.e.} it is the sum of the occurrence probabilities of the rules in the cube: 
$$ P_T =  P_K = \sum_{r \in K} p_r .$$
In the particular case in which the tree is a leaf $\ell$, we will denote the occurence probability of the leaf as $P_{\ell}$. 
\end{definition}

Observe that all distinct decisional trees on the same cube occur with the same probability, as the union of the rules associated to the leaves is the same for all trees and it is equal to $K$. 

\medskip

The average cost of a decision tree is a measure of the cost of testing the conditions that we need to check to decide which actions to take 
when rules occur, weighted by the probability that rules occur. We wish this cost to be as small as possible, especially for rules that occur frequently.  It is formally defined in the following way: 


\begin{definition}[Average Cost of a Decision Tree]
Let $R$ be a $L$-cube such that $P_R=1$, let $T$ be a decision tree for $R$ and  ${\cal L}$ be the set  leaves in $T$. The {\em Average cost} of decision tree $T$ is given by 
$$ \overline{cost}(T) = \sum_{\ell \in {\cal L}} P_{\ell} \, path(\ell),$$
{\em i.e.}, the sum, over all the leaves of the tree, of the costs of the root-leaf path multiplied by the probability on any rule in the corresponding leaf to occur. 

An {\em Optimal Average Cost Decision Tree} for $L$-cube $R$ is a decision tree for the cube with minimum average cost (might not be unique). 
\end{definition}



The algorithm that we will describe in the next section finds a decision tree of minimum average cost for the $L$-cube that describes a given {\bf OR}-Decision table. 

\begin{definition}[Tree-compatible partition]
Given a $k$-cube $K$ with dashes in positions in $D_K \subseteq [ 1..L] $ and a partition of $K$ into two sets $K_0$ and $K_1$, the partition is said to be {\em Tree-compatible} if $K_0$ and $K_1$ are both  $(k-1)$-cubes with the $k-1$ dashes in the same positions $D'_{K} \subset D_K$.  Index $i \in D_K \setminus D'_{K}$ is said to {\em distinguish} between $K_0$ and $K_1$. 
\end{definition}

Observe that there are $k$ distinct tree-compatible partition for any $k$-cube $K$, one for each different index in $D_K$. Moreover, the two subcubes have dashes in the same positions give by set $D_K \setminus \{ i\}$, where index $i$ distinguishes between the two subcubes. All rules of one subcube have condition in position $i$ setted to zero, while those in the other subcube have that condition setted to one. 

